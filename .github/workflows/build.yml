name: ðŸ§  Viral Shorts â€” AI Brain (LLM-only Enforcement)

on:
  workflow_dispatch:
    inputs:
      video_url:
        description: 'URL Video MP4'
        required: true
        default: 'https://api.vidssave.com/api/contentsite_api/media/download_redirect?request=sgRpmKL3iNBpyIDr4IgGHBXJiSxcf0EYQuGexs_6KdEalT5ycrs3ffYBEZXevrdUBcrNn6iDw8MfLNTFhlNy7biCIOWf4sDWWBA_S3Gc0ACg1ZPIBzwNN6iyUU-0-PuEErnV6vklMuUiHCyNARF0XKYdjAEXIFyrKF3ytFZL3SURwBQ6zqTIxoinSGmbNRzuDj7GLLN2-qE4j55wtrmuN0sRuMHC3v3eqf14UHFW6OQMmpQ5sIwzvVB6Y5OhROhLgjaA0g8XRTd1z8-rT37YaXo2Y1N59CFMiBNLYoMKlQY-ogHXajXJ3IPsDSTl679jtoOgZpQQY4lq8X-k8lRdgZ5x16iwKPU9AC5d20FwHAFwPaA6sqqj6Sulox0JtTHVQT3xcaUUJowztbrK3N1wClY_bp0pTnhFn4ntpiJhobshFeQwTW0cvTh7u-DnwrB6zF3N3uAfdG9YkBFZZfMIps_fqAN_JZe1TE8Xy6dOcm1cAkp4CQA-6VSb2h75I3aACfjLy872k1vGM7UyG_aS1MqWJUICirs_GRhll-ZswtvtiONJHwWROte6JB-8vU4YEOzzqQAnMb6vwwKFoFArv6XjRP9hX__3UF6yn8R7YYtp9bMuc53-ovjN9sdZoX02'
      num_shorts:
        description: 'Jumlah Klip'
        required: false
        default: '5'
      ai_creativity:
        description: 'Creativity (0.0 - 1.0)'
        required: false
        default: '0.7'

jobs:
  ai-editor-llm-only:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install system packages
        run: |
          sudo apt-get update -y
          sudo apt-get install -y ffmpeg libsm6 libxext6 libgl1 unzip

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -q wheel setuptools
          python -m pip install -q "Pillow==9.5.0"
          python -m pip install -q yt-dlp openai tiktoken
          python -m pip install -q numpy==1.24.0
          python -m pip install -q imageio imageio[ffmpeg]
          python -m pip install -q "langchain==0.1.16" "langchain-core==0.1.44" "langchain-openai==0.0.8"
          python -m pip install -q --upgrade openai-whisper
          python -m pip install -q "moviepy==1.0.3" decorator tqdm proglog
          # debug prints
          python - <<'PY'
import pkg_resources, sys
print("Python", sys.version.split()[0])
for pkg in ["openai-whisper","moviepy","Pillow","langchain","yt_dlp"]:
    try:
        print(pkg, pkg_resources.get_distribution(pkg).version)
    except Exception:
        print(pkg, "not found")
PY

      - name: Create AI Editor Script (LLM-only enforcement)
        run: |
          cat > ai_editor.py << 'EOF'
#!/usr/bin/env python3
import os, sys, json, time, gc, traceback
from pathlib import Path
from typing import List, Dict, Any

WORKDIR = Path("workspace"); WORKDIR.mkdir(exist_ok=True)
OUTDIR = Path("output"); OUTDIR.mkdir(parents=True, exist_ok=True)
VIDEO_PATH = WORKDIR / "source.mp4"

CUSTOM_API_KEY = "Kontolondon"
CUSTOM_BASE_URL = "https://tes-coral.vercel.app/v1/"

VIDEO_URL = os.environ.get("VIDEO_URL")
NUM_CLIPS = int(os.environ.get("NUM_SHORTS", "5"))
TEMPERATURE = float(os.environ.get("AI_CREATIVITY", "0.7"))

MIN_DURATION = 10.0
MAX_DURATION = 90.0
MERGE_GAP = 4.0
MAX_ATTEMPTS = 5  # iterative LLM refinement attempts

# ---------- download ----------
def download_video():
    import subprocess
    print("Downloading video...")
    try:
        subprocess.run(["yt-dlp", VIDEO_URL, "-o", str(VIDEO_PATH), "--force-overwrites", "--quiet"], check=True, stderr=subprocess.DEVNULL)
    except subprocess.CalledProcessError:
        print("yt-dlp failed; falling back to curl...")
        subprocess.run(["curl", "-L", VIDEO_URL, "-o", str(VIDEO_PATH)], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    if not VIDEO_PATH.exists() or VIDEO_PATH.stat().st_size < 1000:
        raise Exception("Failed to download video or file too small.")
    print("Download OK:", VIDEO_PATH, VIDEO_PATH.stat().st_size)

# ---------- transcribe ----------
def transcribe_video():
    print("Transcribing (Whisper medium)...")
    import whisper
    model = whisper.load_model("medium")
    # do not pass language="auto" to avoid unsupported-language errors
    result = model.transcribe(str(VIDEO_PATH), task="transcribe")
    segments = result.get("segments", [])
    print(f"Whisper returned {len(segments)} segments; language={result.get('language','unknown')}")
    return segments, result

# ---------- helper: compute video duration ----------
def get_video_duration() -> float:
    try:
        from moviepy.editor import VideoFileClip
        v = VideoFileClip(str(VIDEO_PATH))
        dur = float(v.duration)
        v.close()
        return dur
    except Exception as e:
        print("Warning: cannot read video duration via moviepy:", e)
        return None

# ---------- LLM iterative enforcement ----------
def validate_clips(clips: List[Dict[str,Any]], video_duration: float) -> List[str]:
    errors = []
    if not isinstance(clips, list):
        errors.append("Top-level 'clips' is not a list.")
        return errors
    if len(clips) != NUM_CLIPS:
        errors.append(f"Number of clips returned ({len(clips)}) != required NUM_CLIPS ({NUM_CLIPS}).")
    for i, c in enumerate(clips):
        prefix = f"Clip[{i}]"
        try:
            s = float(c.get("start_time", None))
            e = float(c.get("end_time", None))
        except Exception:
            errors.append(f"{prefix}: start_time/end_time not numeric or missing.")
            continue
        if s < 0:
            errors.append(f"{prefix}: start_time < 0.")
        if e <= s:
            errors.append(f"{prefix}: end_time <= start_time.")
        dur = e - s
        if dur < MIN_DURATION:
            errors.append(f"{prefix}: duration {dur:.2f}s < MIN_DURATION {MIN_DURATION}s.")
        if dur > MAX_DURATION:
            errors.append(f"{prefix}: duration {dur:.2f}s > MAX_DURATION {MAX_DURATION}s.")
        if video_duration:
            if s > video_duration or e > video_duration:
                errors.append(f"{prefix}: times out of video range (video_duration={video_duration}s).")
    # check overlaps minimal (allow small overlaps)
    sorted_clips = sorted([ (float(c.get("start_time")), float(c.get("end_time"))) for c in clips if c.get("start_time") is not None ], key=lambda x:x[0]) if clips else []
    for a,b in zip(sorted_clips, sorted_clips[1:]):
        if a[1] > b[0] + 0.01:
            errors.append(f"Overlap detected between {a} and {b}.")
    return errors

def format_transcript_for_prompt(segments: List[Dict[str,Any]], max_chars=12000) -> str:
    lines = []
    for seg in segments[:400]:
        s = seg.get("start",0)
        mm = int(s)//60; ss = int(s)%60
        txt = seg.get("text","").replace("\n"," ").strip()
        lines.append(f"[{mm:02d}:{ss:02d}] {txt}")
    joined = "\n".join(lines)
    if len(joined) > max_chars:
        return joined[:max_chars]
    return joined

def llm_invoke_with_retry(prompt_messages):
    # wrapper to call LLM (LangChain ChatOpenAI invoke)
    from langchain_openai import ChatOpenAI
    llm = ChatOpenAI(temperature=TEMPERATURE, model="gpt-4", openai_api_key=CUSTOM_API_KEY, openai_api_base=CUSTOM_BASE_URL, max_tokens=2000)
    # llm.invoke returns a response object with .content in existing codebase
    resp = llm.invoke(prompt_messages)
    return resp.content if hasattr(resp, "content") else resp

def iterative_llm_clip_selection(segments, video_duration):
    transcript_snippet = format_transcript_for_prompt(segments)
    # Strict prompt header
    base_instruction = f"""
You are a professional short-video editor AI. Return exactly {NUM_CLIPS} clips as JSON only, no commentary, no code fences.
Each clip object MUST contain:
- start_time (decimal seconds)
- end_time (decimal seconds)
- title (string)
- viral_reasoning (string)
- emotion (one of: funny, surprising, informative, emotional, neutral)

Constraints (obligatory):
- Each clip duration MUST be between {MIN_DURATION} and {MAX_DURATION} seconds (inclusive).
- start_time >= 0 and end_time <= {video_duration if video_duration else 'VIDEO_DURATION'}.
- Clips should not overlap (small overlaps <=0.5s tolerated).
- Use the provided TRANSCRIPT to pick clips with coherent hooks & context. Prefer segments containing a hook in first 3 seconds of the clip.
- If any returned clips violate constraints, you MUST fix them when asked; do not output anything except corrected JSON.
- Do NOT ask for clarification; produce the corrected JSON.

Provide output exactly in this form:
{{
  "clips": [
    {{
      "start_time": 12.3,
      "end_time": 34.5,
      "title": "Short title",
      "viral_reasoning": "Why this will be viral, 1-2 sentences",
      "emotion": "surprising"
    }}
  ]
}}
"""
    # initial detailed prompt including transcript and example
    example_json = {
        "clips":[
            {
                "start_time": 12.0,
                "end_time": 28.0,
                "title": "Example Hook",
                "viral_reasoning": "Strong surprising hook + payoff",
                "emotion": "surprising"
            }
        ]
    }
    prompt_template = base_instruction + "\nTRANSCRIPT:\n" + transcript_snippet + "\n\nReturn json now."

    # Build initial message using ChatPromptTemplate style expected by llm.invoke (older integration)
    from langchain_core.prompts import ChatPromptTemplate
    prompt = ChatPromptTemplate.from_template(prompt_template)
    messages = prompt.format_messages()

    last_errors = None
    for attempt in range(1, MAX_ATTEMPTS+1):
        print(f"[LLM ATTEMPT {attempt}/{MAX_ATTEMPTS}] Invoking LLM...")
        try:
            content = llm_invoke_with_retry(messages)
        except Exception as e:
            print("LLM invoke error:", e)
            content = ""
        # strip code fences if any
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
        # try parse
        parsed = None
        try:
            parsed = json.loads(content)
        except Exception as e:
            print("Parse error of LLM output; content head:", (content[:1000] if content else "EMPTY"))
            parsed = None

        if parsed is None:
            # ask LLM to reformat strictly; include original content as 'previous'
            repair_instruction = f"""
Previous LLM output could not be parsed as valid JSON. Please return ONLY the required JSON object under the constraints previously specified.
If you included any commentary or markdown, remove it. Use the TRANSCRIPT context and produce exactly {NUM_CLIPS} valid clips.
Return JSON only.
"""
            prompt_repair = base_instruction + "\nTRANSCRIPT:\n" + transcript_snippet + "\n\nPREVIOUS_OUTPUT:\n" + (content or "EMPTY") + "\n\n" + repair_instruction
            prompt_obj = ChatPromptTemplate.from_template(prompt_repair)
            messages = prompt_obj.format_messages()
            last_errors = ["parse_error"]
            print("Asking LLM to reformat output to strict JSON and obey constraints...")
            continue

        # validate parsed["clips"]
        clips_candidate = parsed.get("clips", [])
        errors = validate_clips(clips_candidate, video_duration)
        if not errors:
            print(f"LLM output VALID on attempt {attempt}.")
            # final acceptance
            return clips_candidate
        else:
            print(f"LLM output INVALID (attempt {attempt}):")
            for e in errors:
                print(" -", e)
            last_errors = errors
            # prepare repair prompt that includes previous JSON and explicit list of errors to fix
            error_list_text = "\n".join(f"- {e}" for e in errors)
            repair_prompt = base_instruction + "\nTRANSCRIPT:\n" + transcript_snippet + "\n\nPREVIOUS_JSON:\n" + json.dumps(parsed, ensure_ascii=False, indent=2) + "\n\nERRORS:\n" + error_list_text + "\n\nTASK: Please FIX the PREVIOUS_JSON so that ALL ERRORS are resolved. Return only the corrected JSON object. Do not add any comments."
            prompt_obj = ChatPromptTemplate.from_template(repair_prompt)
            messages = prompt_obj.format_messages()
            # loop to next attempt
            continue

    # if we exit loop, LLM failed to produce valid clips
    print("LLM failed to produce valid clips after maximum attempts.")
    if last_errors:
        print("Last errors:", last_errors)
    # Fail explicitly (no deterministic local fallback since requirement: clips must come from AI)
    raise Exception("LLM did not produce valid clips after iterative refinement. See logs for details.")

# ---------- cut & save (same safe release) ----------
def safe_release_clip(clip):
    try:
        if hasattr(clip, "close"): clip.close()
    except Exception: pass
    try:
        if hasattr(clip, "reader") and clip.reader is not None: 
            try: clip.reader.close()
            except Exception: pass
    except Exception: pass
    try:
        if hasattr(clip, "audio") and clip.audio is not None and hasattr(clip.audio, "reader"):
            try: clip.audio.reader.close_proc()
            except Exception: pass
    except Exception: pass

def cut_and_save(clips_data):
    print("Cutting and saving", len(clips_data), "clips...")
    from moviepy.editor import VideoFileClip, CompositeVideoClip, ColorClip
    source_video = VideoFileClip(str(VIDEO_PATH))
    for idx, clip in enumerate(clips_data):
        try:
            start = float(clip["start_time"]); end = float(clip["end_time"])
            if end > source_video.duration:
                end = source_video.duration
            subclip = source_video.subclip(start, end)
            # simple convert to vertical 1080x1920 with safe resize
            def resize_with_fallback(c, width=None, height=None):
                try:
                    from moviepy.video.fx.all import resize as moviepy_resize
                    return moviepy_resize(c, width=width, height=height)
                except Exception:
                    from PIL import Image
                    import numpy as np
                    def resize_frame(frame):
                        pil = Image.fromarray(frame)
                        ow,oh = pil.size
                        if width and height:
                            new = (width, height)
                        elif width:
                            ratio = width/ow; new = (width, int(oh*ratio))
                        elif height:
                            ratio = height/oh; new = (int(ow*ratio), height)
                        else:
                            return frame
                        r = pil.resize(new, Image.Resampling.LANCZOS)
                        return np.array(r)
                    return c.fl_image(resize_frame)
            def convert_to_vertical(c):
                w,h = c.size; target_w,target_h = 1080,1920
                scale = target_h / h
                new_w = int(w * scale)
                resized = resize_with_fallback(c, width=new_w, height=target_h)
                if new_w > target_w:
                    x_center = (new_w - target_w)//2
                    return resized.crop(x1=x_center, y1=0, x2=x_center+target_w, y2=target_h)
                else:
                    background = ColorClip(size=(target_w,target_h), color=(0,0,0), duration=c.duration)
                    x_pos = (target_w - new_w)//2
                    return CompositeVideoClip([background, resized.set_position((x_pos,0))])
            vertical = convert_to_vertical(subclip)
            fname = f"short_{idx+1:02d}_{clip.get('emotion','neutral')}.mp4"
            out_path = OUTDIR / fname
            vertical.write_videofile(str(out_path), codec='libx264', audio_codec='aac', fps=30, preset='fast', audio_bitrate='128k', threads=4, verbose=False, logger=None)
            # release
            try: safe_release_clip(vertical); safe_release_clip(subclip)
            except Exception: pass
            gc.collect()
            try: os.sync()
            except Exception: pass
            time.sleep(1)
            # metadata
            meta = {"filename": fname, "title": clip.get("title",""), "viral_reasoning": clip.get("viral_reasoning",""), "emotion": clip.get("emotion","neutral"), "timing":{"start": start, "end": end, "duration": end-start}, "resolution":"1080x1920"}
            with open(OUTDIR / f"short_{idx+1:02d}_meta.json","w", encoding="utf-8") as f:
                json.dump(meta, f, ensure_ascii=False, indent=2)
            print("Saved", fname)
        except Exception as e:
            print("Error cutting clip:", e)
            traceback.print_exc()
    try: source_video.close()
    except Exception: pass

# ---------- main ----------
def main():
    print("=== VIRAL SHORTS LLM-ONLY RUN ===")
    try:
        download_video()
        segments, whisper_all = transcribe_video()
        print(f"Transcription segments: {len(segments)}")
        video_dur = get_video_duration()
        print("Video duration:", video_dur)
        # iterative LLM selection: must produce valid clips by itself
        clips = iterative_llm_clip_selection(segments, video_dur)
        print("LLM produced clips:", clips)
        # cut&save
        cut_and_save(clips)
        print("Done. Output at", OUTDIR.absolute())
    except Exception as e:
        print("Fatal error:", e)
        traceback.print_exc()
        # Fail explicitly so CI shows failure (no deterministic non-LLM fallback)
        sys.exit(2)

if __name__ == '__main__':
    main()
EOF

      - name: Run AI Pipeline
        env:
          VIDEO_URL: ${{ github.event.inputs.video_url }}
          NUM_SHORTS: ${{ github.event.inputs.num_shorts }}
          AI_CREATIVITY: ${{ github.event.inputs.ai_creativity }}
        run: |
          python -c "import PIL; print('Pillow', PIL.__version__)"
          python -c "import moviepy; print('MoviePy', moviepy.__version__)"
          python ai_editor.py

      - name: Zip Results (and test)
        id: zip_results
        run: |
          set -e
          sync || true
          sleep 2
          if [ -d output ] && [ "$(ls -A output)" ]; then
            zip -r viral_shorts_pack.zip output/
            echo "Testing ZIP integrity..."
            if unzip -t viral_shorts_pack.zip >/dev/null 2>&1; then
              echo "zip_ok=true" >> $GITHUB_OUTPUT
              echo "ZIP test passed."
            else
              echo "zip_ok=false" >> $GITHUB_OUTPUT
              echo "ZIP test failed. Will upload folder as fallback."
              rm -f viral_shorts_pack.zip || true
            fi
          else
            echo "zip_ok=false" >> $GITHUB_OUTPUT
            echo "No output files to zip."
          fi

      - name: Upload ZIP artifact (if zip OK)
        if: steps.zip_results.outputs.zip_ok == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: AI-Custom-Shorts-ZIP
          path: viral_shorts_pack.zip
          retention-days: 7

      - name: Upload output folder artifact (fallback)
        if: steps.zip_results.outputs.zip_ok != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: AI-Custom-Shorts-Output
          path: output/
          retention-days: 7

      - name: Final listing
        run: |
          echo "=== Final output listing ==="
          ls -l output || true
