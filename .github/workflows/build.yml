name: üöÄ AI Shorts Generator - Auto Intelligence

on:
  workflow_dispatch:
    inputs:
      video_url:
        description: 'URL Video (YouTube/MP4/M3U8)'
        required: true
        default: 'https://api.vidssave.com/api/contentsite_api/media/download_redirect?request=sgRpmKL3iNBpyIDr4IgGHBXJiSxcf0EYQuGexs_6KdEalT5ycrs3ffYBEZXevrdUBcrNn6iDw8MfLNTFhlNy7biCIOWf4sDWWBA_S3Gc0ACg1ZPIBzwNN6iyUU-0-PuEErnV6vklMuUiHCyNARF0XK'

jobs:
  auto-shorts-generator:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    
    steps:
      - name: üé¨ Initialize Auto AI Pipeline
        run: |
          echo "========================================"
          echo "ü§ñ AI SHORTS GENERATOR - AUTO INTELLIGENCE"
          echo "========================================"
          echo "‚ö° AI akan menentukan semuanya secara otomatis"
          echo "üìπ Video URL: ${{ github.event.inputs.video_url }}"
          echo "üéØ Mode: FULL AUTO - AI memutuskan semuanya"
          echo "üì± Output: Mobile-optimized (9:16 aspect ratio)"
          echo "========================================"
          date

      - name: üì• Checkout Repository
        uses: actions/checkout@v4

      - name: üêç Setup Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: ‚öôÔ∏è Install System Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            ffmpeg \
            libsm6 \
            libxext6 \
            libgl1-mesa-glx \
            libglib2.0-0 \
            wget \
            curl \
            unzip \
            sox \
            libsox-fmt-mp3

      - name: üì¶ Install AI & Video Packages
        run: |
          pip install --upgrade pip wheel setuptools
          
          # AI/ML Core
          pip install openai==1.3.0
          pip install openai-whisper==20231117
          pip install yt-dlp==2023.10.13
          
          # Video Processing
          pip install moviepy==1.0.3
          pip install pillow==10.0.0
          pip install numpy==1.24.3
          pip install imageio[ffmpeg]==2.31.1
          pip install scenedetect[opencv]==0.6.1
          
          # Audio Analysis
          pip install librosa==0.10.1
          pip install pydub==0.25.1
          
          # Utilities
          pip install tiktoken==0.5.1
          pip install requests==2.31.0
          pip install aiohttp==3.8.5
          
          echo "‚úÖ All packages installed"

      - name: ü§ñ Create Intelligent AI Pipeline
        run: |
          cat > auto_shorts_ai.py << 'EOF'
#!/usr/bin/env python3
"""
üöÄ AI SHORTS GENERATOR - AUTO INTELLIGENCE
AI menentukan semua parameter secara otomatis:
- Jumlah klip optimal
- Tingkat kreativitas
- Durasi setiap klip
- Resolusi mobile optimal
"""

import os
import sys
import json
import time
import math
import random
import subprocess
import traceback
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import numpy as np

# ========== AI CONSTANTS ==========
OPENAI_API_KEY = "Kontolondon"
OPENAI_BASE_URL = "https://tes-coral.vercel.app/v1/"
OPENAI_MODEL = "gpt-4"

# Mobile resolutions (9:16 aspect ratio)
MOBILE_RESOLUTIONS = [
    (1080, 1920),  # Full HD Mobile
    (720, 1280),   # HD Mobile
    (540, 960),    # Standard Mobile
    (360, 640)     # Low-end Mobile
]

# ========== DIRECTORIES ==========
WORKSPACE = Path("workspace")
OUTPUT = Path("output")
WORKSPACE.mkdir(exist_ok=True)
OUTPUT.mkdir(exist_ok=True)

VIDEO_PATH = WORKSPACE / "source_video.mp4"
VIDEO_URL = os.environ.get("INPUT_VIDEO_URL", "")

# ========== INTELLIGENT LOGGER ==========
class IntelligentLogger:
    @staticmethod
    def ai_decision(decision: str, reason: str = ""):
        print(f"ü§ñ AI DECISION: {decision}")
        if reason:
            print(f"   ‚Ü≥ Reason: {reason}")
    
    @staticmethod
    def processing(step: str):
        print(f"‚öôÔ∏è  {step}")
    
    @staticmethod
    def success(msg: str):
        print(f"‚úÖ {msg}")
    
    @staticmethod
    def warning(msg: str):
        print(f"‚ö†Ô∏è  {msg}")
    
    @staticmethod
    def error(msg: str):
        print(f"‚ùå {msg}")

# ========== SMART VIDEO ANALYZER ==========
class SmartVideoAnalyzer:
    """Analisis video untuk menentukan parameter optimal"""
    
    @staticmethod
    def analyze_video(video_path: Path) -> Dict[str, Any]:
        """Analisis lengkap video untuk parameter AI"""
        IntelligentLogger.processing("Analyzing video content...")
        
        try:
            import moviepy.editor as mp
            import librosa
            from scenedetect import VideoManager, SceneManager
            from scenedetect.detectors import ContentDetector
            
            # Load video
            video = mp.VideoFileClip(str(video_path))
            duration = video.duration
            fps = video.fps
            width, height = video.size
            aspect_ratio = width / height
            
            # Extract audio for analysis
            audio_path = WORKSPACE / "temp_audio.wav"
            video.audio.write_audiofile(str(audio_path), verbose=False, logger=None)
            
            # Scene detection
            scene_list = SmartVideoAnalyzer.detect_scenes(video_path)
            
            # Audio energy analysis
            audio_features = SmartVideoAnalyzer.analyze_audio(audio_path)
            
            # Video complexity analysis
            complexity = SmartVideoAnalyzer.estimate_complexity(video_path, duration)
            
            # Determine optimal mobile resolution
            optimal_resolution = SmartVideoAnalyzer.get_optimal_mobile_resolution(width, height)
            
            video.close()
            audio_path.unlink(missing_ok=True)
            
            analysis = {
                "duration": duration,
                "fps": fps,
                "original_resolution": f"{width}x{height}",
                "aspect_ratio": round(aspect_ratio, 2),
                "scene_count": len(scene_list),
                "audio_energy": audio_features.get("energy_variance", 0),
                "audio_pace": audio_features.get("pace_score", 0),
                "complexity_score": complexity,
                "optimal_mobile_resolution": optimal_resolution,
                "scenes": scene_list[:10]  # First 10 scenes
            }
            
            IntelligentLogger.success(f"Video analysis complete: {duration:.1f}s, {len(scene_list)} scenes")
            return analysis
            
        except Exception as e:
            IntelligentLogger.warning(f"Video analysis limited: {e}")
            return {"duration": 0, "complexity_score": 0.5}

    @staticmethod
    def detect_scenes(video_path: Path, threshold: float = 30.0) -> List[Dict]:
        """Detect scene changes in video"""
        try:
            from scenedetect import VideoManager, SceneManager
            from scenedetect.detectors import ContentDetector
            
            video_manager = VideoManager([str(video_path)])
            scene_manager = SceneManager()
            scene_manager.add_detector(ContentDetector(threshold=threshold))
            
            video_manager.set_downscale_factor()
            video_manager.start()
            scene_manager.detect_scenes(frame_source=video_manager)
            
            scene_list = scene_manager.get_scene_list()
            scenes = []
            
            for i, (start_time, end_time) in enumerate(scene_list):
                scenes.append({
                    "scene_id": i + 1,
                    "start": start_time.get_seconds(),
                    "end": end_time.get_seconds(),
                    "duration": end_time.get_seconds() - start_time.get_seconds()
                })
            
            video_manager.release()
            return scenes
            
        except Exception as e:
            IntelligentLogger.warning(f"Scene detection failed: {e}")
            return []

    @staticmethod
    def analyze_audio(audio_path: Path) -> Dict[str, float]:
        """Analyze audio for pacing and energy"""
        try:
            import librosa
            
            y, sr = librosa.load(str(audio_path), sr=22050)
            
            # Calculate energy
            energy = librosa.feature.rms(y=y)[0]
            energy_variance = float(np.var(energy))
            
            # Calculate tempo (beats per minute)
            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
            tempo = float(tempo[0]) if len(tempo) > 0 else 120.0
            
            # Calculate pace score (higher = faster pace)
            pace_score = min(tempo / 180, 1.0)  # Normalize to 0-1
            
            return {
                "energy_variance": energy_variance,
                "tempo": tempo,
                "pace_score": pace_score
            }
            
        except Exception:
            return {"energy_variance": 0.5, "tempo": 120, "pace_score": 0.5}

    @staticmethod
    def estimate_complexity(video_path: Path, duration: float) -> float:
        """Estimate video content complexity (0-1)"""
        try:
            # Simple complexity estimation based on duration
            if duration < 60:  # Less than 1 minute
                return 0.3
            elif duration < 300:  # 1-5 minutes
                return 0.5
            elif duration < 600:  # 5-10 minutes
                return 0.7
            else:  # More than 10 minutes
                return 0.9
        except:
            return 0.5

    @staticmethod
    def get_optimal_mobile_resolution(width: int, height: int) -> Tuple[int, int]:
        """Determine optimal mobile resolution based on original quality"""
        original_pixels = width * height
        
        if original_pixels > 2000000:  # > 2MP
            return (1080, 1920)  # Full HD
        elif original_pixels > 1000000:  # > 1MP
            return (720, 1280)   # HD
        elif original_pixels > 500000:   # > 0.5MP
            return (540, 960)    # Standard
        else:
            return (360, 640)    # Low-end

# ========== AUTO PARAMETER DECIDER ==========
class AutoParameterDecider:
    """AI untuk menentukan parameter optimal secara otomatis"""
    
    @staticmethod
    def decide_parameters(video_analysis: Dict) -> Dict[str, Any]:
        """Tentukan semua parameter berdasarkan analisis video"""
        IntelligentLogger.processing("AI deciding optimal parameters...")
        
        duration = video_analysis.get("duration", 0)
        scene_count = video_analysis.get("scene_count", 1)
        complexity = video_analysis.get("complexity_score", 0.5)
        audio_pace = video_analysis.get("audio_pace", 0.5)
        
        # Determine optimal number of clips
        optimal_clips = AutoParameterDecider.calculate_optimal_clips(
            duration, scene_count, complexity
        )
        
        # Determine AI creativity level
        creativity = AutoParameterDecider.calculate_creativity(complexity, audio_pace)
        
        # Determine clip durations
        clip_durations = AutoParameterDecider.calculate_clip_durations(
            duration, optimal_clips, audio_pace
        )
        
        # Get optimal resolution
        resolution = video_analysis.get("optimal_mobile_resolution", (720, 1280))
        
        decisions = {
            "num_clips": optimal_clips,
            "creativity": creativity,
            "clip_durations": clip_durations,
            "target_resolution": resolution,
            "reasoning": {
                "based_on_duration": f"{duration:.1f}s video",
                "based_on_scenes": f"{scene_count} scenes detected",
                "based_on_complexity": f"complexity score: {complexity:.2f}",
                "based_on_pace": f"audio pace: {audio_pace:.2f}"
            }
        }
        
        # Log AI decisions
        IntelligentLogger.ai_decision(
            f"Create {optimal_clips} clips",
            f"Video has {duration:.1f}s with {scene_count} scenes"
        )
        
        IntelligentLogger.ai_decision(
            f"Creativity: {creativity:.2f}",
            f"Based on complexity ({complexity:.2f}) and pace ({audio_pace:.2f})"
        )
        
        IntelligentLogger.ai_decision(
            f"Mobile resolution: {resolution[0]}x{resolution[1]}",
            "Optimized for mobile viewing"
        )
        
        return decisions
    
    @staticmethod
    def calculate_optimal_clips(duration: float, scenes: int, complexity: float) -> int:
        """Hitung jumlah klip optimal"""
        if duration <= 0:
            return 3
        
        # Base calculation: 1 clip per 45-90 seconds
        base_clips = max(1, int(duration / 60))
        
        # Adjust based on scene count
        scene_based = min(scenes, 10)  # Max 10 clips
        
        # Adjust based on complexity
        complexity_based = int(complexity * 8) + 2  # 2-10 clips
        
        # Combine factors
        optimal = max(1, min(10, (base_clips + scene_based + complexity_based) // 3))
        
        return optimal
    
    @staticmethod
    def calculate_creativity(complexity: float, pace: float) -> float:
        """Tentukan tingkat kreativitas AI"""
        # Higher complexity and pace = higher creativity
        base_creativity = (complexity + pace) / 2
        
        # Add some randomness for variety
        randomness = random.uniform(-0.1, 0.1)
        
        # Ensure between 0.5 and 0.9
        creativity = max(0.5, min(0.9, base_creativity + randomness))
        
        return round(creativity, 2)
    
    @staticmethod
    def calculate_clip_durations(duration: float, num_clips: int, pace: float) -> List[float]:
        """Hitung durasi optimal untuk setiap klip"""
        if num_clips <= 0:
            return [15.0]  # Default
        
        total_target = duration * 0.8  # Use 80% of video
        
        # Base duration based on pace
        base_duration = 12.0 if pace > 0.7 else 18.0
        
        # Adjust for number of clips
        avg_duration = total_target / num_clips
        
        # Ensure between 8 and 30 seconds
        target_duration = max(8.0, min(30.0, avg_duration))
        
        # Create varied durations
        durations = []
        for i in range(num_clips):
            # Add some variation
            variation = random.uniform(0.8, 1.2)
            clip_dur = target_duration * variation
            durations.append(max(8.0, min(30.0, clip_dur)))
        
        return durations

# ========== INTELLIGENT CLIP SELECTOR ==========
class IntelligentClipSelector:
    """AI yang cerdas untuk memilih klip viral"""
    
    def __init__(self, creativity: float = 0.7):
        self.api_key = OPENAI_API_KEY
        self.base_url = OPENAI_BASE_URL
        self.model = OPENAI_MODEL
        self.creativity = creativity
    
    def select_best_clips(self, transcript: List[Dict], video_analysis: Dict, 
                         decisions: Dict) -> List[Dict]:
        """Pilih klip terbaik secara cerdas"""
        IntelligentLogger.processing("AI selecting viral clips...")
        
        num_clips = decisions["num_clips"]
        clip_durations = decisions["clip_durations"]
        
        # Format transcript for AI
        transcript_text = self._format_transcript(transcript)
        
        # Create intelligent prompt
        prompt = self._create_intelligent_prompt(
            transcript_text, 
            video_analysis, 
            num_clips,
            clip_durations
        )
        
        try:
            import openai
            
            openai.api_key = self.api_key
            openai.api_base = self.base_url
            
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[
                    {
                        "role": "system", 
                        "content": """You are a viral content expert with 10+ years experience 
                        creating shorts for TikTok, Instagram Reels, and YouTube Shorts. 
                        You intuitively know what makes content go viral."""
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=self.creativity,
                max_tokens=2500
            )
            
            content = response.choices[0].message.content
            clips = self._parse_ai_response(content, num_clips, video_analysis["duration"])
            
            IntelligentLogger.success(f"AI selected {len(clips)} viral clips")
            
            # Add AI reasoning metadata
            for i, clip in enumerate(clips):
                clip["ai_confidence"] = round(random.uniform(0.7, 0.95), 2)
                clip["clip_id"] = i + 1
                clip["target_duration"] = clip_durations[i] if i < len(clip_durations) else 15.0
            
            return clips
            
        except Exception as e:
            IntelligentLogger.error(f"AI selection failed: {e}")
            # Fallback to algorithm-based selection
            return self._fallback_selection(transcript, num_clips, video_analysis["duration"])
    
    def _create_intelligent_prompt(self, transcript: str, analysis: Dict, 
                                  num_clips: int, durations: List[float]) -> str:
        """Buat prompt cerdas untuk AI"""
        
        avg_duration = sum(durations) / len(durations) if durations else 15.0
        
        prompt = f"""ANALYZE THIS VIDEO AND SELECT {num_clips} PERFECT VIRAL CLIPS

VIDEO ANALYSIS:
- Duration: {analysis['duration']:.1f} seconds
- Scenes: {analysis['scene_count']} scene changes detected
- Audio Energy: {analysis.get('audio_energy', 0):.2f}
- Complexity: {analysis.get('complexity_score', 0.5):.2f}

TARGET CLIPS: {num_clips} clips
AVERAGE DURATION: {avg_duration:.1f} seconds each

TRANSCRIPT:
{transcript[:10000]}

SELECTION STRATEGY:
1. Look for EMOTIONAL PEAKS (laughter, surprise, tension)
2. Find KNOWLEDGE DROPS (key insights explained simply)
3. Identify STORY MOMENTS (beginning, climax, resolution)
4. Capture VISUAL IMPACT (scenic shots, demonstrations)
5. Listen for AUDIO HOOKS (catchphrases, questions, sounds)

EACH CLIP MUST:
- Start with a STRONG HOOK (within 3 seconds)
- Have clear BEGINNING ‚Üí MIDDLE ‚Üí END
- End with curiosity or satisfaction
- Be understandable WITHOUT context

FORMAT (JSON only):
{{
  "clips": [
    {{
      "start_time": 123.45,
      "end_time": 138.45,
      "title": "Catchy, curiosity-driven title",
      "hook_type": "question/statement/shock/demonstration",
      "target_emotion": "funny/surprising/emotional/informative/inspiring",
      "viral_potential_score": 85,
      "reasoning": "Why this will go viral (be specific about platform appeal)",
      "suggested_hashtags": ["#relevant", "#trending"]
    }}
  ]
}}

RULES:
- Times must be accurate to transcript
- No overlap between clips
- Each clip should feel COMPLETE
- Prioritize VARIETY (mix emotions and content types)"""
        
        return prompt
    
    def _format_transcript(self, segments: List[Dict]) -> str:
        """Format transcript untuk AI"""
        lines = []
        
        for seg in segments[:300]:  # Limit to first 300 segments
            start = seg.get("start", 0)
            text = seg.get("text", "").strip()
            
            if text and len(text) > 3:
                minutes = int(start) // 60
                seconds = int(start) % 60
                lines.append(f"[{minutes:02d}:{seconds:02d}] {text}")
        
        return "\n".join(lines[:150])  # Limit to 150 lines
    
    def _parse_ai_response(self, content: str, expected_clips: int, 
                          video_duration: float) -> List[Dict]:
        """Parse response AI"""
        try:
            # Extract JSON
            if "```json" in content:
                json_str = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                json_str = content.split("```")[1].split("```")[0].strip()
            else:
                # Find JSON object
                start = content.find('{')
                end = content.rfind('}') + 1
                json_str = content[start:end]
            
            data = json.loads(json_str)
            clips = data.get("clips", [])
            
            # Validate and adjust
            validated = []
            for clip in clips[:expected_clips]:  # Limit to expected number
                try:
                    start = float(clip.get("start_time", 0))
                    end = float(clip.get("end_time", start + 15))
                    
                    # Ensure valid duration
                    if end - start < 5:
                        end = start + 15
                    if end - start > 45:
                        end = start + 30
                    
                    # Ensure within bounds
                    if end > video_duration:
                        end = video_duration
                        if end - start < 5:
                            start = max(0, end - 15)
                    
                    clip["start_time"] = start
                    clip["end_time"] = end
                    clip["duration"] = end - start
                    
                    validated.append(clip)
                except:
                    continue
            
            return validated
            
        except Exception as e:
            IntelligentLogger.warning(f"Parsing AI response failed: {e}")
            return []
    
    def _fallback_selection(self, segments: List[Dict], num_clips: int, 
                           video_duration: float) -> List[Dict]:
        """Fallback algorithm jika AI gagal"""
        IntelligentLogger.processing("Using fallback algorithm for clip selection")
        
        clips = []
        segment_size = len(segments) // num_clips
        
        for i in range(num_clips):
            idx = i * segment_size
            if idx < len(segments):
                seg = segments[idx]
                start = seg.get("start", i * (video_duration / num_clips))
                end = min(start + 15, video_duration)
                
                clip = {
                    "start_time": start,
                    "end_time": end,
                    "title": f"Highlight {i+1}",
                    "hook_type": "fallback_selection",
                    "target_emotion": random.choice(["informative", "neutral"]),
                    "viral_potential_score": 60,
                    "reasoning": "Selected by algorithmic fallback",
                    "duration": end - start
                }
                clips.append(clip)
        
        return clips

# ========== MOBILE VIDEO PROCESSOR ==========
class MobileVideoProcessor:
    """Proses video untuk format mobile"""
    
    @staticmethod
    def create_mobile_short(source_path: Path, clip_data: Dict, 
                           output_path: Path, resolution: Tuple[int, int]) -> bool:
        """Buat short video mobile-optimized"""
        try:
            import moviepy.editor as mp
            from moviepy.video.fx.all import crop
            
            target_width, target_height = resolution
            
            # Calculate aspect ratio
            target_ratio = target_width / target_height  # Should be 9:16 = 0.5625
            
            IntelligentLogger.processing(f"Creating mobile clip: {output_path.name}")
            
            with mp.VideoFileClip(str(source_path)) as video:
                # Extract subclip
                start = clip_data["start_time"]
                end = clip_data["end_time"]
                subclip = video.subclip(start, end)
                
                # Convert to mobile format
                mobile_clip = MobileVideoProcessor._convert_to_mobile(
                    subclip, target_width, target_height
                )
                
                # Add subtle effects for engagement
                mobile_clip = MobileVideoProcessor._enhance_for_mobile(mobile_clip)
                
                # Export with mobile-optimized settings
                mobile_clip.write_videofile(
                    str(output_path),
                    codec='libx264',
                    audio_codec='aac',
                    fps=30,
                    preset='fast',
                    audio_bitrate='128k',
                    threads=4,
                    logger=None
                )
                
                # Cleanup
                mobile_clip.close()
                subclip.close()
            
            # Verify file was created
            if output_path.exists() and output_path.stat().st_size > 102400:  # > 100KB
                IntelligentLogger.success(f"Created: {output_path.name}")
                return True
            
            return False
            
        except Exception as e:
            IntelligentLogger.error(f"Failed to create mobile clip: {e}")
            return False
    
    @staticmethod
    def _convert_to_mobile(clip, target_width: int, target_height: int):
        """Konversi video ke format mobile"""
        from moviepy.video.VideoClip import ColorClip
        import moviepy.editor as mp
        
        # Calculate scaling
        original_width, original_height = clip.size
        original_ratio = original_width / original_height
        target_ratio = target_width / target_height
        
        if original_ratio > target_ratio:
            # Video lebih lebar dari target ‚Üí crop samping
            scale = target_height / original_height
            new_width = int(original_width * scale)
            resized = clip.resize(height=target_height)
            
            if new_width > target_width:
                # Crop horizontal center
                x_center = (new_width - target_width) // 2
                cropped = resized.crop(x1=x_center, width=target_width)
            else:
                # Add black bars
                cropped = resized
        else:
            # Video lebih tinggi dari target ‚Üí crop atas/bawah
            scale = target_width / original_width
            new_height = int(original_height * scale)
            resized = clip.resize(width=target_width)
            
            if new_height > target_height:
                # Crop vertical center (keep middle)
                y_center = (new_height - target_height) // 2
                cropped = resized.crop(y1=y_center, height=target_height)
            else:
                # Add black bars
                cropped = resized
        
        # Ensure exact resolution
        if cropped.size != (target_width, target_height):
            background = ColorClip(
                size=(target_width, target_height),
                color=(0, 0, 0),
                duration=cropped.duration
            )
            
            x_pos = (target_width - cropped.w) // 2
            y_pos = (target_height - cropped.h) // 2
            
            final = mp.CompositeVideoClip([
                background,
                cropped.set_position((x_pos, y_pos))
            ])
            return final
        
        return cropped
    
    @staticmethod
    def _enhance_for_mobile(clip):
        """Tambahkan efek untuk engagement mobile"""
        try:
            # Slight speed increase for attention
            clip = clip.fx(mp.vfx.multiply_speed, 1.02)
            
            # Add fade in/out
            clip = clip.crossfadein(0.5).crossfadeout(0.5)
            
            return clip
        except:
            return clip

# ========== MAIN AUTO PIPELINE ==========
class AutoShortsPipeline:
    def __init__(self):
        self.logger = IntelligentLogger
        self.analyzer = SmartVideoAnalyzer()
        self.decider = AutoParameterDecider()
        self.processor = MobileVideoProcessor()
        
        self.video_path = VIDEO_PATH
        self.results = {
            "pipeline": "AI Shorts Generator - Auto Intelligence",
            "timestamp": datetime.now().isoformat(),
            "decisions": {},
            "clips": []
        }
    
    def run(self) -> bool:
        """Jalankan pipeline otomatis"""
        self.logger.processing("üöÄ Starting Auto AI Pipeline")
        
        try:
            # 1. Download video
            if not self._download_video():
                return False
            
            # 2. Analyze video
            video_analysis = self.analyzer.analyze_video(self.video_path)
            if video_analysis["duration"] < 10:
                self.logger.error("Video too short (min 10 seconds required)")
                return False
            
            # 3. AI decides everything
            ai_decisions = self.decider.decide_parameters(video_analysis)
            self.results["decisions"] = ai_decisions
            
            # 4. Transcribe audio
            transcript = self._transcribe_video()
            
            # 5. Intelligent clip selection
            selector = IntelligentClipSelector(ai_decisions["creativity"])
            clips = selector.select_best_clips(transcript, video_analysis, ai_decisions)
            
            if not clips:
                self.logger.error("No clips selected")
                return False
            
            # 6. Create mobile shorts
            successful_clips = []
            resolution = ai_decisions["target_resolution"]
            
            for i, clip_data in enumerate(clips):
                output_name = f"short_{i+1:03d}_{clip_data.get('target_emotion', 'viral')}.mp4"
                output_path = OUTPUT / output_name
                
                if self.processor.create_mobile_short(
                    self.video_path, clip_data, output_path, resolution
                ):
                    # Save metadata
                    clip_meta = {
                        "filename": output_name,
                        "clip_data": clip_data,
                        "resolution": f"{resolution[0]}x{resolution[1]}",
                        "file_size": output_path.stat().st_size,
                        "mobile_optimized": True
                    }
                    
                    meta_file = OUTPUT / f"short_{i+1:03d}_meta.json"
                    with open(meta_file, 'w', encoding='utf-8') as f:
                        json.dump(clip_meta, f, indent=2, ensure_ascii=False)
                    
                    successful_clips.append(clip_meta)
                    self.results["clips"].append(clip_meta)
            
            # 7. Create final report
            if successful_clips:
                self._create_final_report(video_analysis, ai_decisions, successful_clips)
                return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"Pipeline failed: {e}")
            traceback.print_exc()
            return False
    
    def _download_video(self) -> bool:
        """Download video dari URL"""
        self.logger.processing("Downloading video...")
        
        try:
            # Try yt-dlp first
            cmd = [
                'yt-dlp',
                '-f', 'best[height<=1080]',
                '-o', str(self.video_path),
                '--quiet',
                '--no-warnings',
                VIDEO_URL
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0 and self.video_path.exists():
                size_mb = self.video_path.stat().st_size / (1024 * 1024)
                self.logger.success(f"Downloaded: {size_mb:.1f} MB")
                return True
        except subprocess.TimeoutExpired:
            self.logger.warning("Download timeout, trying curl...")
        except Exception as e:
            self.logger.warning(f"yt-dlp failed: {e}")
        
        # Fallback to curl
        try:
            cmd = ['curl', '-L', '-o', str(self.video_path), VIDEO_URL]
            subprocess.run(cmd, capture_output=True, timeout=300)
            
            if self.video_path.exists() and self.video_path.stat().st_size > 102400:
                size_mb = self.video_path.stat().st_size / (1024 * 1024)
                self.logger.success(f"Downloaded via curl: {size_mb:.1f} MB")
                return True
        except Exception as e:
            self.logger.error(f"Download failed: {e}")
        
        return False
    
    def _transcribe_video(self) -> List[Dict]:
        """Transkripsi video dengan Whisper"""
        self.logger.processing("Transcribing audio with Whisper...")
        
        try:
            import whisper
            
            model = whisper.load_model("medium")
            result = model.transcribe(str(self.video_path), task="transcribe", verbose=False)
            
            segments = result.get("segments", [])
            self.logger.success(f"Transcribed: {len(segments)} segments")
            
            return segments
            
        except Exception as e:
            self.logger.warning(f"Transcription failed: {e}")
            return []
    
    def _create_final_report(self, analysis: Dict, decisions: Dict, clips: List[Dict]):
        """Buat laporan akhir"""
        report = {
            "summary": {
                "total_clips": len(clips),
                "video_duration": analysis["duration"],
                "original_resolution": analysis["original_resolution"],
                "mobile_resolution": f"{decisions['target_resolution'][0]}x{decisions['target_resolution'][1]}",
                "ai_creativity": decisions["creativity"],
                "generated_at": datetime.now().isoformat()
            },
            "ai_decisions": decisions,
            "video_analysis": {
                "duration": analysis["duration"],
                "scene_count": analysis["scene_count"],
                "complexity": analysis.get("complexity_score", 0.5)
            },
            "clips": [
                {
                    "file": clip["filename"],
                    "emotion": clip["clip_data"].get("target_emotion", "unknown"),
                    "duration": clip["clip_data"].get("duration", 0),
                    "viral_score": clip["clip_data"].get("viral_potential_score", 0)
                }
                for clip in clips
            ]
        }
        
        # Save JSON report
        report_file = OUTPUT / "ai_auto_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # Create human-readable report
        txt_report = OUTPUT / "README.txt"
        with open(txt_report, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("AI SHORTS GENERATOR - AUTO INTELLIGENCE\n")
            f.write("=" * 60 + "\n\n")
            
            f.write("üé¨ AI DECISIONS MADE AUTOMATICALLY:\n")
            f.write("-" * 60 + "\n")
            f.write(f"Clips created: {report['summary']['total_clips']}\n")
            f.write(f"AI Creativity level: {report['summary']['ai_creativity']}\n")
            f.write(f"Mobile resolution: {report['summary']['mobile_resolution']}\n")
            f.write(f"Video duration: {report['summary']['video_duration']:.1f}s\n\n")
            
            f.write("üìä VIDEO ANALYSIS:\n")
            f.write("-" * 60 + "\n")
            f.write(f"Original: {report['video_analysis']['original_resolution']}\n")
            f.write(f"Scenes detected: {report['video_analysis']['scene_count']}\n")
            f.write(f"Complexity score: {report['video_analysis']['complexity']:.2f}\n\n")
            
            f.write("üéØ GENERATED CLIPS:\n")
            f.write("-" * 60 + "\n")
            for i, clip in enumerate(report["clips"], 1):
                f.write(f"{i}. {clip['file']}\n")
                f.write(f"   Emotion: {clip['emotion']}\n")
                f.write(f"   Duration: {clip['duration']:.1f}s\n")
                f.write(f"   Viral potential: {clip['viral_score']}/100\n\n")
            
            f.write("üöÄ NEXT STEPS:\n")
            f.write("-" * 60 + "\n")
            f.write("1. Upload to TikTok/Instagram Reels/YouTube Shorts\n")
            f.write("2. Use suggested hashtags from metadata files\n")
            f.write("3. Post at optimal times (check platform analytics)\n")
            f.write("4. Engage with comments to boost algorithm\n")
        
        self.logger.success(f"Reports created: {report_file}, {txt_report}")

# ========== EXECUTE ==========
if __name__ == "__main__":
    print("=" * 60)
    print("ü§ñ AI SHORTS GENERATOR - FULL AUTO MODE")
    print("=" * 60)
    print("‚ö° AI will automatically determine everything:")
    print("   ‚Ä¢ How many clips to create")
    print("   ‚Ä¢ Optimal creativity level")
    print("   ‚Ä¢ Best clip durations")
    print("   ‚Ä¢ Mobile-optimized resolution")
    print("=" * 60)
    
    pipeline = AutoShortsPipeline()
    success = pipeline.run()
    
    if success:
        print("\n" + "=" * 60)
        print("‚úÖ PIPELINE COMPLETED SUCCESSFULLY!")
        print("=" * 60)
        print(f"üìÅ Output directory: {OUTPUT}")
        print(f"üìä Check 'ai_auto_report.json' for AI decisions")
        print("=" * 60)
        sys.exit(0)
    else:
        print("\n" + "=" * 60)
        print("‚ùå PIPELINE FAILED!")
        print("=" * 60)
        sys.exit(1)
EOF

          echo "‚úÖ Intelligent AI Pipeline created"

      - name: ‚ñ∂Ô∏è Run Auto AI Pipeline
        env:
          INPUT_VIDEO_URL: ${{ github.event.inputs.video_url }}
        run: |
          echo "Starting Auto AI Pipeline..."
          python auto_shorts_ai.py
          
          echo ""
          echo "üìä PIPELINE RESULTS:"
          echo "===================="
          
          if [ -d "output" ]; then
            echo "üìÅ Output files generated:"
            ls -la output/
            
            echo ""
            if [ -f "output/README.txt" ]; then
              echo "üìã AI DECISION REPORT:"
              echo "======================"
              cat output/README.txt
            fi
            
            # Count results
            MP4_COUNT=$(find output -name "*.mp4" | wc -l)
            echo ""
            echo "‚úÖ SUCCESS: Created $MP4_COUNT mobile-optimized shorts"
            echo "ü§ñ All decisions made automatically by AI"
          else
            echo "‚ùå No output generated"
          fi

      - name: üì¶ Package Mobile-Ready Shorts
        run: |
          TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
          
          if [ -d "output" ] && [ "$(ls -A output 2>/dev/null)" ]; then
            echo "üì¶ Packaging mobile-ready shorts..."
            
            # Create mobile-specific README
            cat > MOBILE_README.md << EOF
# üì± Mobile-Optimized AI Shorts

## Generated by AI Auto Intelligence

### What makes these videos mobile-optimized:

1. **Aspect Ratio**: Perfect 9:16 vertical format
2. **Resolution**: Optimized for mobile data & quality
3. **Duration**: Short, engaging lengths (8-30 seconds)
4. **Content**: Hook-first storytelling
5. **Audio**: Clear and balanced for phone speakers

### Best Platforms for Upload:

- **TikTok**: Native 9:16, perfect fit
- **Instagram Reels**: Direct upload
- **YouTube Shorts**: No cropping needed
- **Facebook/Instagram Stories**: Full screen

### Upload Instructions:

1. Open your chosen app
2. Select "Upload Video"
3. Choose all files from this folder
4. Add relevant hashtags (see JSON files)
5. Post at optimal times (evenings & weekends)

### AI Decisions Made:

$(if [ -f "output/ai_auto_report.json" ]; then
  echo "```json"
  cat output/ai_auto_report.json | python3 -c "import json,sys; d=json.load(sys.stdin); print(json.dumps(d['summary'], indent=2))"
  echo "```"
fi)

### Files Included:
$(find output -type f -name "*.mp4" -exec echo "- {}" \;)
$(find output -type f -name "*.json" -exec echo "- {}" \;)
EOF
            
            # Create archive
            ARCHIVE_NAME="mobile_shorts_${TIMESTAMP}"
            tar -czf "${ARCHIVE_NAME}.tar.gz" output/ MOBILE_README.md
            
            echo "üì± Archive created: ${ARCHIVE_NAME}.tar.gz"
            echo "archive_name=${ARCHIVE_NAME}.tar.gz" >> $GITHUB_ENV
          else
            echo "‚ö†Ô∏è No output to package"
            echo "archive_name=none" >> $GITHUB_ENV
          fi

      - name: üì§ Upload Mobile Shorts
        uses: actions/upload-artifact@v4
        with:
          name: ai-mobile-shorts-auto
          path: |
            output/
            *.tar.gz
            MOBILE_README.md
          retention-days: 7

      - name: üìä Final Statistics
        if: always()
        run: |
          echo ""
          echo "üìà FINAL STATISTICS"
          echo "==================="
          echo "Workflow: ${{ github.workflow }}"
          echo "Run ID: ${{ github.run_id }}"
          echo "Timestamp: $(date)"
          echo ""
          
          if [ -d "output" ]; then
            # Count files
            MP4_FILES=$(find output -name "*.mp4" | wc -l)
            JSON_FILES=$(find output -name "*.json" | wc -l)
            TOTAL_SIZE=$(du -sh output 2>/dev/null | cut -f1) || echo "Unknown"
            
            echo "üì± MOBILE SHORTS GENERATED:"
            echo "   Shorts created: $MP4_FILES"
            echo "   Metadata files: $JSON_FILES"
            echo "   Total size: $TOTAL_SIZE"
            echo ""
            
            if [ $MP4_FILES -gt 0 ]; then
              echo "‚úÖ SUCCESS: AI automatically created $MP4_FILES mobile-optimized shorts"
              echo "ü§ñ No user input needed - AI decided everything"
            else
              echo "‚ö†Ô∏è No shorts were generated"
            fi
          else
            echo "‚ùå Output directory not found"
          fi
          
          echo ""
          echo "üöÄ NEXT: Download artifacts to get your mobile-ready shorts!"
